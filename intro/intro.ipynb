{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API Call Example\n",
    "\n",
    "This notebook demonstrates how to make a simple call to the OpenAI API to generate text using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T01:01:22.222803Z",
     "start_time": "2024-03-26T01:01:20.437756Z"
    }
   },
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (1.14.2)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from openai) (4.3.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from openai) (2.6.4)\r\n",
      "Requirement already satisfied: sniffio in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from openai) (4.66.2)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from openai) (4.10.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\r\n",
      "Requirement already satisfied: python-dotenv in /Users/claudioed/development/github/llm/llm-with-openai/.venv/lib/python3.9/site-packages (1.0.1)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T01:01:24.932622Z",
     "start_time": "2024-03-26T01:01:22.224444Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "  What is chat-gpt??\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  messages=[        \n",
    "         {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat-GPT is a conversational AI model developed by OpenAI that is designed to generate human-like responses in natural language conversations. It is based on the GPT-3 (Generative Pre-trained Transformer 3) model and is trained on a large dataset of text from the internet to generate coherent and contextually relevant responses to user inputs. Chat-GPT can be used in a variety of applications, such as chatbots, virtual assistants, and customer service interactions.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Open AI - API Parameters explanation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T01:01:24.937658Z",
     "start_time": "2024-03-26T01:01:24.934559Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in April 2023, when using the OpenAI API, particularly for the ChatGPT or similar models, you interact with the API by sending HTTP requests. The specific parameters you use can significantly affect the behavior and output of the model. Below, I'll explain some of the key parameters based on the documentation available at that time. Please note that the API and its documentation may have evolved, so it's always a good idea to check the latest documentation for the most current information.\n",
      "\n",
      "### 1. `model`\n",
      "\n",
      "- **Description**: This parameter specifies which version of the model you want to use. OpenAI provides various models like `text-davinci-003`, `gpt-3.5-turbo`, etc., each with different capabilities, performance, and cost implications.\n",
      "- **Example Usage**: `\"model\": \"text-davinci-003\"`\n",
      "\n",
      "### 2. `prompt`\n",
      "\n",
      "- **Description**: The `prompt` parameter is the input text that you want the model to respond to. It can be a question, a statement, or any text to which you want the model to generate a continuation or response.\n",
      "- **Example Usage**: `\"prompt\": \"Explain the theory of relativity\"`\n",
      "\n",
      "### 3. `max_tokens`\n",
      "\n",
      "- **Description**: This parameter controls the maximum length of the generated response. The length is measured in tokens, where a token can be a word or part of a word. The actual number of words or characters that correspond to a specific number of tokens can vary, as tokens are determined by the model's tokenizer.\n",
      "- **Example Usage**: `\"max_tokens\": 150`\n",
      "\n",
      "### 4. `temperature`\n",
      "\n",
      "- **Description**: The `temperature` parameter affects the randomness of the output. A lower temperature (close to 0) makes the model's responses more deterministic and repetitive, while a higher temperature (closer to 1) makes the responses more varied and creative.\n",
      "- **Example Usage**: `\"temperature\": 0.7`\n",
      "\n",
      "### 5. `top_p`\n",
      "\n",
      "- **Description**: This parameter, also known as nucleus sampling, controls the diversity of the generated text by limiting the model to consider only the top `p` percent of probability mass when generating tokens. It's another way to control the randomness and creativity of the output.\n",
      "- **Example Usage**: `\"top_p\": 0.9`\n",
      "\n",
      "### 6. `frequency_penalty` and `presence_penalty`\n",
      "\n",
      "- **Description**: These parameters help to fine-tune the output by penalizing the repetition of words (`frequency_penalty`) and the repetition of topics or information already mentioned (`presence_penalty`). Adjusting these can help make the output more diverse or prevent the model from getting stuck on certain topics.\n",
      "- **Example Usage**: \n",
      "  - `\"frequency_penalty\": 0.5`\n",
      "  - `\"presence_penalty\": 0.5`\n",
      "\n",
      "### 7. `stop`\n",
      "\n",
      "- **Description**: The `stop` parameter allows you to define one or more stop sequences. When the model generates one of these sequences, it will stop generating further text. This is useful for controlling the length of the output or ensuring the model stops at a logical endpoint.\n",
      "- **Example Usage**: `\"stop\": [\"\\n\", \"END\"]`\n",
      "\n",
      "### Example Request\n",
      "\n",
      "Here's how a simple request might look using some of these parameters:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"prompt\": \"What is the significance of the Turing Test in artificial intelligence?\",\n",
      "  \"max_tokens\": 100,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_p\": 1,\n",
      "  \"frequency_penalty\": 0,\n",
      "  \"presence_penalty\": 0\n",
      "}\n",
      "```\n",
      "\n",
      "This request would ask the model to explain the significance of the Turing Test, with a moderately creative response that's up to 100 tokens long.\n",
      "\n",
      "Remember, the best way to understand how these parameters affect the output is to experiment with them. Also, keep an eye on the OpenAI documentation for any updates or new features.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "  I'm learning openAI api, could you explain how to use the parameters for usage? You can find the documentation at https://platform.openai.com/docs/api-reference/chat\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  messages=[        \n",
    "         {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "    ],\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T01:02:17.696093Z",
     "start_time": "2024-03-26T01:01:24.939057Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T01:02:17.709355Z",
     "start_time": "2024-03-26T01:02:17.706357Z"
    }
   },
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
